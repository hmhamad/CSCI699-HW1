{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzaaAopZfKOc"
      },
      "source": [
        "## Loading & tokenizing datasets from raw jsonl for fine-tuning pre-trained RoBERTA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5faY1dYBfHfN",
        "outputId": "05dd899b-cc68-423b-c120-65453909802c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "# of total sentences: 1688 139\n",
            "An example sentence: She evaluates 3,000 German verbs with a token frequency between 10 and 2,000 against the Duden ( Dudenredaktion 2001 ) .\n",
            "# of labels: 6\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "TRAIN_PATH = \"/content/drive/MyDrive/CS699/data/ACL-ARC_train.jsonl\"\n",
        "TEST_PATH = \"/content/drive/MyDrive/CS699/data/ACL-ARC_test.jsonl\"\n",
        "\n",
        "def load_json(json_path, text_key='text', label_key='label'):\n",
        "  X = []\n",
        "  Y = []\n",
        "  with open(json_path, 'r') as f:\n",
        "    for line in f:\n",
        "      raw_json = json.loads(line)\n",
        "      X.append(raw_json[text_key])\n",
        "      Y.append(raw_json[label_key])\n",
        "  return X, Y\n",
        "\n",
        "trainX, trainY = load_json(TRAIN_PATH)\n",
        "testX, testY = load_json(TEST_PATH)\n",
        "num_classes = len(set(trainY))\n",
        "\n",
        "print(\"# of total sentences:\", len(trainX), len(testX))\n",
        "print(\"An example sentence:\", trainX[2])\n",
        "print('# of labels:', num_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5mRIW5_o91_"
      },
      "source": [
        "- As a base case, pre-trained RoBERTa and its base tokenizer will be used to tokenizer our training dataset and assign int index for each of the token\n",
        "- pre-trained RoBERTa weights and base tokenizers are used from the ones provided in Huggingface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-bTx8DZqpjNM",
        "outputId": "e921099f-5524-4bc2-ad18-24f899099646"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.16.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.47)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.11.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.10.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yk8tGLF9pUEj",
        "outputId": "650aed66-5dd6-4680-c590-7ff7fef9d81d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([    0, 42702,  2156,    81,     5,   375,   367,   107,  2156,   552,\n",
            "           19,  9766,    11,     5,   304,     9,  2239,     8, 17325,  6448,\n",
            "           13,  3857,     9,   455, 28564,   268,    36,  5415,  2156,  7528,\n",
            "        25606,   732,  4422, 20082,  2156,  7528,   102, 25606,   732,  4422,\n",
            "        20082,  2156,  7528,   428, 25606, 12041,   282,  1115,  3994,  3592,\n",
            "         2156,  7528,  4839,  2156,  1233,  2017,    34,    57,   156,    15,\n",
            "            5,   304,     9, 17325,  2239,  6448,     7,  5281, 16762, 46563,\n",
            "         8117, 45774, 28201, 22810,    50,  1617,    14,  4064,    11,    10,\n",
            "        45774, 28201,  1291,    36,  2197,  2156, 11151, 25606,  3513, 18086,\n",
            "            8,  7380,  2156,  7969, 25606, 19021, 22704,  4400,  1076,     4,\n",
            "         2156,  6708, 25606,  5866,   324,     8, 13891,  2156,  6708, 25606,\n",
            "         6760,  3979,  4400,  1076,     4,  2156,  6193, 25606, 14687,   219,\n",
            "          677,   260,  1638,     8, 13880,  2156,  5155, 25606, 19443,  9649,\n",
            "          329,  4400,  1076,     4,  2156,  6193, 25606,   255, 40435,  1636,\n",
            "        18002,     8, 19443,  9649,   329,  2156,  3788,  4839,   479,     2,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1])\n",
            "tensor(0)\n"
          ]
        }
      ],
      "source": [
        "from sklearn import preprocessing\n",
        "import torch\n",
        "from transformers import RobertaTokenizer\n",
        "\n",
        "def convert_txt2tokenid(tokenizer, text):\n",
        "  token_ids = []\n",
        "  for sent in text:\n",
        "    token_ids.append(tokenizer.encode(sent, padding='max_length', return_tensors = 'pt'))\n",
        "  return torch.cat(token_ids, dim=0)\n",
        "\n",
        "roberta_tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
        "\n",
        "encoded_trainX = convert_txt2tokenid(roberta_tokenizer, trainX)\n",
        "encoded_testX = convert_txt2tokenid(roberta_tokenizer, testX)\n",
        "\n",
        "label_encoder = preprocessing.LabelEncoder()\n",
        "\n",
        "encoded_trainY = label_encoder.fit_transform(trainY)\n",
        "encoded_testY = label_encoder.transform(testY)\n",
        "\n",
        "encoded_trainY = torch.tensor(encoded_trainY)\n",
        "encoded_testY = torch.tensor(encoded_testY)\n",
        "\n",
        "print(encoded_trainX[0])\n",
        "print(encoded_trainY[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIC1gqWHiauv"
      },
      "source": [
        "## Creating a dataloader for both training and test datasets\n",
        "- Batch size set as indicated in the paper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xigcjb4nQ5wl"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
        "\n",
        "batch_size = 16\n",
        "\n",
        "train_dataset = TensorDataset(encoded_trainX, encoded_trainY)\n",
        "train_dataloader = DataLoader(\n",
        "            train_dataset,\n",
        "            sampler = RandomSampler(train_dataset),\n",
        "            batch_size = batch_size\n",
        "        )\n",
        "\n",
        "test_dataset = TensorDataset(encoded_testX, encoded_testY)\n",
        "test_dataloader = DataLoader(\n",
        "            test_dataset,\n",
        "            sampler = RandomSampler(test_dataset),\n",
        "            batch_size = batch_size\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQQBIQ1WXaWd",
        "outputId": "4740a5ce-5277-4e1d-91e2-b8675b58c4ae"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "CustomRoberta(\n",
              "  (robert): RobertaModel(\n",
              "    (embeddings): RobertaEmbeddings(\n",
              "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
              "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
              "      (token_type_embeddings): Embedding(1, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): RobertaEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): RobertaPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (linear): Linear(in_features=768, out_features=6, bias=True)\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (activation): Tanh()\n",
              ")"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import RobertaModel, RobertaForSequenceClassification\n",
        "from torch import nn\n",
        "\n",
        "# RobertaForSequenceClassification could also be used.\n",
        "# Drop out rate as used in the paper\n",
        "class CustomRoberta(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "          super(CustomRoberta, self).__init__()\n",
        "          self.robert = RobertaModel.from_pretrained(\"roberta-base\", output_attentions = True, output_hidden_states = True)\n",
        "          self.linear = nn.Linear(768, num_classes)\n",
        "          self.dropout = nn.Dropout(0.1)\n",
        "          self.activation = nn.Tanh()\n",
        "\n",
        "    def forward(self, ids):\n",
        "          # index 1 represents the pooled_output, the cls token.\n",
        "          sequence_output = self.robert(ids)[1]\n",
        "          \n",
        "          linear_output = self.linear(sequence_output)\n",
        "          dropout = self.dropout(linear_output)\n",
        "          output = self.activation(dropout)\n",
        "\n",
        "          return output\n",
        "\n",
        "model = CustomRoberta(6) #num_classes=num_classes)\n",
        "model.cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9gJ2Tn6maNuH"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# from Umang's code -- Note don't use Frequent Direction --> Buffer size is too large\n",
        "class FrequentDirectionAccountant:\n",
        "    \"\"\"\n",
        "    Frequent Directions algorithm (Alg 2 from the paper) for streaming SVD.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, k, l, n, device):\n",
        "        \"\"\"\n",
        "        :param k: number of eigen vectors we want eventually (k should be less than l+1)\n",
        "        :param l: buffer size\n",
        "        :param n: number of parameters/dimension of vector\n",
        "        :param device:\n",
        "        \"\"\"\n",
        "        self.K = k\n",
        "        self.L = l\n",
        "        self.N = n\n",
        "\n",
        "        self.step = 0\n",
        "        self.buffer = torch.zeros(self.L, self.N, device=device)\n",
        "\n",
        "    def update(self, vector):\n",
        "        \"\"\"\n",
        "        run one step of Freq Direction\n",
        "        :param vector:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "\n",
        "        self.buffer[self.L - 1] = vector\n",
        "        _, S, Vt = torch.linalg.svd(self.buffer, full_matrices=False)\n",
        "        delta = S[-1] ** 2\n",
        "        new_svd_vals = torch.sqrt(torch.clip(S ** 2 - delta, min=0, max=None))\n",
        "        self.buffer = torch.diag(new_svd_vals) @ Vt\n",
        "        self.step += 1\n",
        "\n",
        "    def get_current_buffer(self):\n",
        "        return self.buffer\n",
        "\n",
        "    def get_current_directions(self):\n",
        "        \"\"\"return top k eigen vectors of A^TA\"\"\"\n",
        "        _, _, Vt_B = torch.linalg.svd(self.buffer, full_matrices=False)\n",
        "        return Vt_B[:self.K]\n",
        "\n",
        "    def get_current_buffer(self):\n",
        "        return self.buffer\n",
        "\n",
        "    def get_current_directions(self):\n",
        "        \"\"\"return top k eigen vectors of A^TA\"\"\"\n",
        "        _, _, Vt_B = torch.linalg.svd(self.buffer, full_matrices=False)\n",
        "        return Vt_B[:self.K]\n",
        "\n",
        "def count_params(model: torch.nn.Module, skip_bn_bias=False):\n",
        "    count = 0\n",
        "    for param in model.parameters():\n",
        "        if param.requires_grad:\n",
        "            if param.dim() <= 1 and skip_bn_bias:\n",
        "                pass\n",
        "            else:\n",
        "                count += param.numel()\n",
        "    return count\n",
        "\n",
        "def flatten_grads(model, num_params, skip_bn_bias=False):\n",
        "    flat_grads = torch.zeros(num_params, requires_grad=False)\n",
        "    idx = 0\n",
        "    for param in model.parameters():\n",
        "        if param.requires_grad:\n",
        "            if param.dim() <= 1 and skip_bn_bias:\n",
        "                pass\n",
        "            else:\n",
        "                flat_grads[idx:idx + param.numel()] = torch.flatten(param.grad).data.cpu()\n",
        "                idx += param.numel()\n",
        "    return flat_grads\n",
        "\n",
        "def get_loss_value(model, loader, device):\n",
        "    \"\"\"\n",
        "    Evaluation loop for the multi-class classification problem.\n",
        "    return (loss, accuracy)\n",
        "    \"\"\"\n",
        "\n",
        "    model.eval()\n",
        "    losses = []\n",
        "    accuracies = []\n",
        "    with torch.no_grad():\n",
        "        for i, (images, labels) in enumerate(loader):\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(images)\n",
        "            loss = torch.nn.functional.cross_entropy(outputs, labels, reduce=None).detach()\n",
        "            losses.append(loss.reshape(-1))\n",
        "\n",
        "            acc = (torch.argmax(outputs, dim=1) == labels).float().detach()\n",
        "            accuracies.append(acc.reshape(-1))\n",
        "\n",
        "        losses = torch.cat(losses, dim=0).mean().cpu().data.numpy()\n",
        "        accuracies = torch.cat(accuracies, dim=0).mean().cpu().data.numpy()\n",
        "        return losses, accuracies\n",
        "\n",
        "def flatten_params(model, num_params, skip_bn_bias=False):\n",
        "    flat_param = torch.zeros(num_params, requires_grad=False)\n",
        "    idx = 0\n",
        "    for param in model.parameters():\n",
        "        if param.requires_grad:\n",
        "            if param.dim() <= 1 and skip_bn_bias:\n",
        "                pass\n",
        "            else:\n",
        "                flat_param[idx:idx + param.numel()] = torch.flatten(param).data.cpu()\n",
        "                idx += param.numel()\n",
        "    return flat_param\n",
        "\n",
        "def set_weights_by_direction(model, x, y, direction1, direction2, weights, skip_bn_bias=False):\n",
        "    if direction2 is not None:\n",
        "        changes = direction1 * x + direction2 * y\n",
        "    else:\n",
        "        changes = direction1 * x\n",
        "\n",
        "    apply_params(model, weights + changes, skip_bn_bias=skip_bn_bias)\n",
        "\n",
        "def apply_params(model, array, skip_bn_bias=False):\n",
        "    idx = 0\n",
        "    for param in model.parameters():\n",
        "        if param.requires_grad:\n",
        "            if param.dim() <= 1 and skip_bn_bias:\n",
        "                pass\n",
        "            else:\n",
        "                param.data = (array[idx:idx + param.numel()]).reshape(param.data.shape)\n",
        "                idx += param.numel()\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txbixdiF4gtf"
      },
      "source": [
        "## Fine-tuning raw RoBERTa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CLbiVSn49CU9",
        "outputId": "5f555906-1482-4e13-c443-789a3882d0b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.3729814 0.51079136\n",
            "1.3610529 0.51079136\n",
            "1.3447055 0.51079136\n",
            "1.3034331 0.5611511\n",
            "1.3793471 0.51079136\n",
            "1.4034746 0.51079136\n",
            "1.3905021 0.51079136\n",
            "1.3779043 0.51079136\n",
            "1.3885386 0.51079136\n",
            "1.3740332 0.51079136\n",
            "959.2429268360138\n"
          ]
        }
      ],
      "source": [
        "import dill\n",
        "import os\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import AdamW\n",
        "\n",
        "# hyperparameters as set by the paper\n",
        "epochs = 10\n",
        "optimizer = AdamW(model.parameters(), lr = 2e-5)\n",
        "\n",
        "RESULT_FOLDER = \"/content/drive/MyDrive/CS699/data/ROBERTA/\"\n",
        "os.makedirs(f\"{RESULT_FOLDER}/ckpt\", exist_ok=True)\n",
        "\n",
        "device = torch.device(\"cuda\")\n",
        "#total_params = count_params(model, skip_bn_bias=True)\n",
        "#fd = FrequentDirectionAccountant(k=2, l=10, n=total_params, device=device)\n",
        "t0 = time.time()\n",
        "for epoch in range(epochs):\n",
        "  model.train()\n",
        "\n",
        "  for i, batch in enumerate(train_dataloader):\n",
        "\n",
        "    d_input_id = batch[0].to(device)\n",
        "    d_labels = batch[1].to(device)\n",
        "    outputs = model(d_input_id)\n",
        "    loss = torch.nn.functional.cross_entropy(outputs, d_labels)\n",
        "\n",
        "    model.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    #fd.update(flatten_grads(model, total_params, skip_bn_bias=True))\n",
        "\n",
        "  loss, acc = get_loss_value(model, test_dataloader, device=device)\n",
        "  print(loss, acc)\n",
        "  torch.save(\n",
        "      model.state_dict(), f'{RESULT_FOLDER}/ckpt/{epoch + 1}_model.pt',\n",
        "      pickle_module=dill\n",
        "  )\n",
        "\n",
        "training_time = time.time() - t0\n",
        "#buffer = fd.get_current_buffer()\n",
        "#directions = fd.get_current_directions()\n",
        "#directions = directions.cpu().data.numpy()\n",
        "'''\n",
        "np.savez(\n",
        "    f\"{RESULT_FOLDER}/buffer.npy\",\n",
        "    buffer=buffer.cpu().data.numpy(), direction1=directions[0], direction2=directions[1]\n",
        ")\n",
        "'''\n",
        "\n",
        "print(training_time)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snYTAJOOrWzP"
      },
      "source": [
        "## Computing directions for projections\n",
        "- Random direction [O]\n",
        "- PCA [O] - works with incremental PCA\n",
        "- In-training SVD [X]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Chh1hg5ZsWaz"
      },
      "outputs": [],
      "source": [
        "def create_normalized_random_direction(model, skip_bn_bias=False):\n",
        "    weights = [param.data for param in model.parameters()]\n",
        "    directions = []\n",
        "\n",
        "    # filter normalization part\n",
        "    for w in weights:\n",
        "        if w.dim() <= 1:\n",
        "            if skip_bn_bias:\n",
        "                pass\n",
        "                # ignore directions for weights with 1 dimension\n",
        "            else:\n",
        "                # this is different from original paper. We just keep sane defaults here\n",
        "                t = torch.randn_like(w)\n",
        "                t.mul_(torch.abs(t) / (torch.abs(w)) + 1e-10)\n",
        "                directions.append(t)\n",
        "        else:\n",
        "            d = torch.randn_like(w)\n",
        "            for filter_d, filter_w in zip(d, w):\n",
        "                filter_d.mul_(filter_w.norm() / (filter_d.norm() + 1e-10))\n",
        "            directions.append(d)\n",
        "    return directions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hUJdQN86tQ5U"
      },
      "outputs": [],
      "source": [
        "direction1 = create_normalized_random_direction(model, skip_bn_bias=True)\n",
        "direction2 = create_normalized_random_direction(model, skip_bn_bias=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jkryzfx9wsMN"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "\n",
        "import dill\n",
        "import numpy\n",
        "from sklearn.decomposition import IncrementalPCA\n",
        "from torch import nn\n",
        "\n",
        "logger = logging.getLogger()\n",
        "\n",
        "def get_PCA_directions(model: nn.Module, state_files, skip_bn_bias):\n",
        "    \"\"\"\n",
        "        Compute PCA direction as defined in Li et al. 2017 (https://arxiv.org/abs/1712.09913)\n",
        "    :param model: model object\n",
        "    :param state_files: list of checkpoints.\n",
        "    :param skip_bn_bias: Skip batch norm and bias while flattening the model params. Li et al. do not use batch norm and bias parameters\n",
        "    :return: (pc1, pc2, explained variance)\n",
        "    \"\"\"\n",
        "\n",
        "    # load final weights and flatten\n",
        "    model.load_state_dict(torch.load(state_files[-1], pickle_module=dill, map_location=\"cpu\"))\n",
        "    total_param = count_params(model, skip_bn_bias=skip_bn_bias)\n",
        "    w_final = flatten_params(model, total_param, skip_bn_bias=skip_bn_bias)\n",
        "\n",
        "    # compute w_i- w_final\n",
        "    w_diff_matrix = numpy.zeros((len(state_files) - 1, total_param))\n",
        "    for idx, file in enumerate(state_files[:-1]):\n",
        "        print(file)\n",
        "        model.load_state_dict(torch.load(file, pickle_module=dill, map_location=\"cpu\"))\n",
        "        w = flatten_params(model, total_param, skip_bn_bias=skip_bn_bias)\n",
        "\n",
        "        diff = w - w_final\n",
        "        w_diff_matrix[idx] = diff\n",
        "        del w\n",
        "        del diff\n",
        "\n",
        "    # Perform PCA on the optimization path matrix\n",
        "    logger.info(\"Perform PCA on the models\")\n",
        "    print(\"Performing PCA\")\n",
        "\n",
        "    # Due to the memory issue, traditional PCA has been replaced by incremental PCA, \n",
        "    # in which the matrix decomposition is done in a batch a\n",
        "    pca = IncrementalPCA(n_components=2)\n",
        "    pca.fit(w_diff_matrix)\n",
        "    pc1 = numpy.array(pca.components_[0])\n",
        "    pc2 = numpy.array(pca.components_[1])\n",
        "    logger.info(\n",
        "        f\"angle between pc1 and pc2: {numpy.dot(pc1, pc2) / (numpy.linalg.norm(pc1) * numpy.linalg.norm(pc2))}\"\n",
        "    )\n",
        "    logger.info(f\"pca.explained_variance_ratio_: {pca.explained_variance_ratio_}\")\n",
        "\n",
        "    return pc1, pc2, pca.explained_variance_ratio_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iD0MGDh2w6Fr",
        "outputId": "9929f1e6-96d6-46df-9dee-3b7b11e044fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/CS699/data/ROBERTA/ckpt/1_model.pt\n",
            "/content/drive/MyDrive/CS699/data/ROBERTA/ckpt/2_model.pt\n",
            "/content/drive/MyDrive/CS699/data/ROBERTA/ckpt/3_model.pt\n",
            "/content/drive/MyDrive/CS699/data/ROBERTA/ckpt/4_model.pt\n",
            "/content/drive/MyDrive/CS699/data/ROBERTA/ckpt/5_model.pt\n",
            "/content/drive/MyDrive/CS699/data/ROBERTA/ckpt/6_model.pt\n",
            "/content/drive/MyDrive/CS699/data/ROBERTA/ckpt/7_model.pt\n",
            "/content/drive/MyDrive/CS699/data/ROBERTA/ckpt/8_model.pt\n",
            "/content/drive/MyDrive/CS699/data/ROBERTA/ckpt/9_model.pt\n",
            "Performing PCA\n"
          ]
        }
      ],
      "source": [
        "import glob\n",
        "import os\n",
        "\n",
        "import numpy\n",
        "\n",
        "CHECKPOINTS_FILES = glob.glob(\"/content/drive/MyDrive/CS699/data/ROBERTA/ckpt/*\")\n",
        "DIRECTION_DIR = \"/content/drive/MyDrive/CS699/data/ROBERTA/directions/\"\n",
        "FILE_NAME = 'pca_directions.npz'\n",
        "\n",
        "os.makedirs(f\"{DIRECTION_DIR}\", exist_ok=True)\n",
        "\n",
        "direction1, direction2, ex_var = get_PCA_directions(model, CHECKPOINTS_FILES, True)\n",
        "\n",
        "numpy.savez(\n",
        "    f\"{DIRECTION_DIR}/{FILE_NAME}\", explained_variance=ex_var,\n",
        "    direction2=direction2, direction1=direction1\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5X1ZPmjRtqgm"
      },
      "source": [
        "## Computing trajectories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DHHksQTNt1PM"
      },
      "outputs": [],
      "source": [
        "import dill\n",
        "import glob\n",
        "\n",
        "import torch\n",
        "import numpy\n",
        "\n",
        "CHECKPOINTS_FILES = glob.glob(\"/content/drive/MyDrive/CS699/data/ROBERTA/ckpt/*\")\n",
        "DIRECTION_DIR = \"/content/drive/MyDrive/CS699/data/ROBERTA/directions/\"\n",
        "FILE_NAME = 'pca_directions.npz'\n",
        "\n",
        "temp = numpy.load(DIRECTION_DIR+FILE_NAME)\n",
        "direction1 = torch.tensor(temp[\"direction1\"], device=\"cpu\").float()\n",
        "direction2 = torch.tensor(temp[\"direction2\"], device=\"cpu\").float()\n",
        "\n",
        "model.load_state_dict(torch.load(CHECKPOINTS_FILES[-1], pickle_module=dill, map_location=\"cpu\"))\n",
        "total_param = count_params(model, skip_bn_bias=True)\n",
        "w_final = flatten_params(model, total_param, skip_bn_bias=True)\n",
        "\n",
        "w_diff_matrix = torch.zeros(len(CHECKPOINTS_FILES) - 1, total_param)\n",
        "for idx, file in enumerate(CHECKPOINTS_FILES[:-1]):\n",
        "    model.load_state_dict(torch.load(file, pickle_module=dill, map_location=\"cpu\"))\n",
        "    w = flatten_params(model, total_param, skip_bn_bias=True)\n",
        "\n",
        "    diff = w - w_final\n",
        "    w_diff_matrix[idx] = diff\n",
        "\n",
        "if torch.isclose(direction1 @ direction2, torch.tensor(0.0)):\n",
        "    logger.info(\"The directions are orthogonal\")\n",
        "    # when dx and dy are orthorgonal\n",
        "    xcoords = w_diff_matrix @ direction1 / direction1.norm()\n",
        "    ycoords = w_diff_matrix @ direction2 / direction2.norm()\n",
        "else:\n",
        "    # w_diff (nxd)\n",
        "    # A = dx2\n",
        "    # X = 2xn\n",
        "    # AX = w_diff.T\n",
        "    # solve the least squre problem: Ax = d\n",
        "    A = torch.vstack([direction1, direction2]).T  # num_param X 2\n",
        "    temp = torch.linalg.lstsq(A, w_diff_matrix.T).solution  # 2\n",
        "    xcoords, ycoords = temp[0], temp[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BqJDfCI-2W0o"
      },
      "outputs": [],
      "source": [
        "PROJECTION_DIR = \"/content/drive/MyDrive/CS699/data/ROBERTA/projections\"\n",
        "FILE_NAME = 'pca_dir_proj.npz'\n",
        "\n",
        "os.makedirs(f\"{PROJECTION_DIR}\", exist_ok=True)\n",
        "\n",
        "numpy.savez(\n",
        "    f\"{PROJECTION_DIR}/{FILE_NAME}\", xcoordinates=xcoords.cpu().data.numpy(),\n",
        "    ycoordinates=ycoords.cpu().data.numpy()\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGfTZ6Xp3HVT"
      },
      "source": [
        "## Computing the loss landscape of the final model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rTOX1DEG3oP3"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "\n",
        "CHECKPOINTS_FILES = glob.glob(\"/content/drive/MyDrive/CS699/data/ROBERTA/ckpt/*\")\n",
        "DIRECTION_DIR = \"/content/drive/MyDrive/CS699/data/ROBERTA/directions/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TDih8O6w3J_0",
        "outputId": "3809c29b-fcdf-4ed5-ae55-e0d3e0b7158a"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x-range: -5.0:30.0:20\n",
            "y-range: -5.0:30.0:20\n",
            "[-5.         -3.15789474 -1.31578947  0.52631579  2.36842105  4.21052632\n",
            "  6.05263158  7.89473684  9.73684211 11.57894737 13.42105263 15.26315789\n",
            " 17.10526316 18.94736842 20.78947368 22.63157895 24.47368421 26.31578947\n",
            " 28.15789474 30.        ]\n",
            "[-5.         -3.15789474 -1.31578947  0.52631579  2.36842105  4.21052632\n",
            "  6.05263158  7.89473684  9.73684211 11.57894737 13.42105263 15.26315789\n",
            " 17.10526316 18.94736842 20.78947368 22.63157895 24.47368421 26.31578947\n",
            " 28.15789474 30.        ]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "x: 30.0000, y: 30.0000, loss:1.4416: 100%|| 400/400 [1:45:54<00:00, 15.89s/it]\n"
          ]
        }
      ],
      "source": [
        "import dill\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = torch.device(\"cuda\")\n",
        "xcoords = \"20:-5:30\" \n",
        "ycoords = \"20:-5:30\"\n",
        "\n",
        "state_dict = torch.load(CHECKPOINTS_FILES[-1], pickle_module=dill, map_location=device)\n",
        "model.load_state_dict(state_dict)\n",
        "\n",
        "total_params = count_params(model, skip_bn_bias=True)\n",
        "pretrained_weights = flatten_params(model, num_params=total_params, skip_bn_bias=True).to(device)\n",
        "\n",
        "temp = numpy.load(DIRECTION_DIR+\"pca_directions.npz\")\n",
        "direction1 = torch.tensor(temp[\"direction1\"], device=device).float()\n",
        "direction2 = torch.tensor(temp[\"direction2\"], device=device).float()\n",
        "\n",
        "x_num, x_min, x_max = [float(i) for i in xcoords.split(\":\")]\n",
        "y_num, y_min, y_max = [float(i) for i in ycoords.split(\":\")]\n",
        "\n",
        "x_num, y_num = int(x_num), int(y_num)\n",
        "\n",
        "print(f\"x-range: {x_min}:{x_max}:{x_num}\")\n",
        "print(f\"y-range: {y_min}:{y_max}:{y_num}\")\n",
        "\n",
        "xcoordinates = numpy.linspace(x_min, x_max, num=x_num)\n",
        "print(xcoordinates)\n",
        "ycoordinates = numpy.linspace(y_min, y_max, num=y_num)\n",
        "print(ycoordinates)\n",
        "\n",
        "losses = numpy.zeros((x_num, y_num))\n",
        "accuracies = numpy.zeros((x_num, y_num))\n",
        "\n",
        "with tqdm(total=x_num * y_num) as pbar:\n",
        "    for idx_x, x in enumerate(xcoordinates):\n",
        "        for idx_y, y in enumerate(ycoordinates):\n",
        "            # import ipdb;ipdb.set_trace()\n",
        "            set_weights_by_direction(\n",
        "                model, x, y, direction1, direction2, pretrained_weights,\n",
        "                skip_bn_bias=True\n",
        "            )\n",
        "            losses[idx_x, idx_y], accuracies[idx_x, idx_y] = get_loss_value(\n",
        "                model, train_dataloader, device\n",
        "            )\n",
        "            pbar.set_description(f\"x:{x: .4f}, y:{y: .4f}, loss:{losses[idx_x, idx_y]:.4f}\")\n",
        "            pbar.update(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jGqo2SV05gWJ"
      },
      "outputs": [],
      "source": [
        "import os \n",
        "\n",
        "LOSS_DIR = \"/content/drive/MyDrive/CS699/data/ROBERTA/loss_surface/\"\n",
        "LOSS_NAME = 'pca_dir_loss_surface_m5p30.npz'\n",
        "\n",
        "os.makedirs(f\"{LOSS_DIR}\", exist_ok=True)\n",
        "\n",
        "numpy.savez(\n",
        "    f\"{LOSS_DIR}/{LOSS_NAME}\", losses=losses, accuracies=accuracies,\n",
        "    xcoordinates=xcoordinates, ycoordinates=ycoordinates\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8QY7THl5DYs"
      },
      "source": [
        "## Plotting results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "JFTvErNV5Foj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "outputId": "942d613f-781f-4836-92cb-e094227b3caa"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import numpy\n",
        "from matplotlib import pyplot\n",
        "\n",
        "LOSS_DIR = \"/content/drive/MyDrive/CS699/data/ROBERTA/loss_surface/\"\n",
        "LOSS_NAME = 'pca_dir_loss_surface_m5p30.npz'\n",
        "\n",
        "PROJECTION_DIR = \"/content/drive/MyDrive/CS699/data/ROBERTA/projections\"\n",
        "FILE_NAME = 'pca_dir_proj.npz'\n",
        "\n",
        "data = numpy.load(f\"{LOSS_DIR + LOSS_NAME}\")\n",
        "\n",
        "xcoords = data[\"xcoordinates\"]\n",
        "ycoords = data[\"ycoordinates\"]\n",
        "losses = data[\"losses\"]\n",
        "acc = data[\"accuracies\"]\n",
        "\n",
        "X, Y = numpy.meshgrid(xcoords, ycoords, indexing=\"ij\")\n",
        "Z = losses\n",
        "fig = pyplot.figure()\n",
        "CS = pyplot.contour(X, Y, Z, cmap='viridis', levels=numpy.arange(1, 3, 0.02))\n",
        "pyplot.clabel(CS, inline=1, fontsize=8)\n",
        "\n",
        "data = numpy.load(PROJECTION_DIR+\"/\"+FILE_NAME)\n",
        "\n",
        "xcoords = data[\"xcoordinates\"]\n",
        "ycoords = data[\"ycoordinates\"]\n",
        "pyplot.plot(xcoords, ycoords, linewidth=10, alpha=0.3)\n",
        "pyplot.colorbar()\n",
        "pyplot.scatter(xcoords, ycoords, marker='X', c=numpy.arange(len(xcoords)))\n",
        "pyplot.tick_params('y', labelsize='x-large')\n",
        "pyplot.tick_params('x', labelsize='x-large')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xcoords"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JsxsdDE4qL8L",
        "outputId": "8ca84753-682f-4604-fa73-8470963009c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-4.181184  , -3.9535398 , -3.4906135 , -2.269921  , -0.4199738 ,\n",
              "       -0.34216294, -0.3023487 , -0.09297835, -0.01179948], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "oP80FA5v8FuD"
      },
      "outputs": [],
      "source": [
        "fig.savefig(\n",
        "    f\"/content/drive/MyDrive/CS699/data/ROBERTA/m5p30_400_trajectory+contour_2d_viridis\", dpi=300,\n",
        "    bbox_inches='tight'\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "roberta_acl-arc_classification.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}