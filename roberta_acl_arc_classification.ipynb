{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "roberta_acl-arc_classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Loading & tokenizing datasets from raw jsonl for fine-tuning pre-trained RoBERTA"
      ],
      "metadata": {
        "id": "mzaaAopZfKOc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "TRAIN_PATH = \"/content/drive/MyDrive/CS699/data/ACL-ARC_train.jsonl\"\n",
        "TEST_PATH = \"/content/drive/MyDrive/CS699/data/ACL-ARC_test.jsonl\"\n",
        "\n",
        "def load_json(json_path, text_key='text', label_key='label'):\n",
        "  X = []\n",
        "  Y = []\n",
        "  with open(json_path, 'r') as f:\n",
        "    for line in f:\n",
        "      raw_json = json.loads(line)\n",
        "      X.append(raw_json[text_key])\n",
        "      Y.append(raw_json[label_key])\n",
        "  return X, Y\n",
        "\n",
        "trainX, trainY = load_json(TRAIN_PATH)\n",
        "testX, testY = load_json(TEST_PATH)\n",
        "num_classes = len(set(trainY))\n",
        "\n",
        "print(\"# of total sentences:\", len(trainX), len(testX))\n",
        "print(\"An example sentence:\", trainX[2])\n",
        "print('# of labels:', num_classes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5faY1dYBfHfN",
        "outputId": "6ba167fd-a64e-4290-a4d0-152ebce96708"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# of total sentences: 1688 139\n",
            "An example sentence: She evaluates 3,000 German verbs with a token frequency between 10 and 2,000 against the Duden ( Dudenredaktion 2001 ) .\n",
            "# of labels: 6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- As a base case, pre-trained RoBERTa and its base tokenizer will be used to tokenizer our training dataset and assign int index for each of the token\n",
        "- pre-trained RoBERTa weights and base tokenizers are used from the ones provided in Huggingface"
      ],
      "metadata": {
        "id": "o5mRIW5_o91_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-bTx8DZqpjNM",
        "outputId": "e2c479c0-a249-4f4c-e52c-1fe05d0a487e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.16.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.11.4)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.10.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.47)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import preprocessing\n",
        "import torch\n",
        "from transformers import RobertaTokenizer\n",
        "\n",
        "def convert_txt2tokenid(tokenizer, text):\n",
        "  token_ids = []\n",
        "  for sent in text:\n",
        "    token_ids.append(tokenizer.encode(sent, padding='max_length', return_tensors = 'pt'))\n",
        "  return torch.cat(token_ids, dim=0)\n",
        "\n",
        "roberta_tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
        "\n",
        "encoded_trainX = convert_txt2tokenid(roberta_tokenizer, trainX)\n",
        "encoded_testX = convert_txt2tokenid(roberta_tokenizer, testX)\n",
        "\n",
        "label_encoder = preprocessing.LabelEncoder()\n",
        "\n",
        "encoded_trainY = label_encoder.fit_transform(trainY)\n",
        "encoded_testY = label_encoder.transform(testY)\n",
        "\n",
        "encoded_trainY = torch.tensor(encoded_trainY)\n",
        "encoded_testY = torch.tensor(encoded_testY)\n",
        "\n",
        "print(encoded_trainX[0])\n",
        "print(encoded_trainY[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yk8tGLF9pUEj",
        "outputId": "3be98bd5-5894-4337-98a8-11bab7be8669"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([    0, 42702,  2156,    81,     5,   375,   367,   107,  2156,   552,\n",
            "           19,  9766,    11,     5,   304,     9,  2239,     8, 17325,  6448,\n",
            "           13,  3857,     9,   455, 28564,   268,    36,  5415,  2156,  7528,\n",
            "        25606,   732,  4422, 20082,  2156,  7528,   102, 25606,   732,  4422,\n",
            "        20082,  2156,  7528,   428, 25606, 12041,   282,  1115,  3994,  3592,\n",
            "         2156,  7528,  4839,  2156,  1233,  2017,    34,    57,   156,    15,\n",
            "            5,   304,     9, 17325,  2239,  6448,     7,  5281, 16762, 46563,\n",
            "         8117, 45774, 28201, 22810,    50,  1617,    14,  4064,    11,    10,\n",
            "        45774, 28201,  1291,    36,  2197,  2156, 11151, 25606,  3513, 18086,\n",
            "            8,  7380,  2156,  7969, 25606, 19021, 22704,  4400,  1076,     4,\n",
            "         2156,  6708, 25606,  5866,   324,     8, 13891,  2156,  6708, 25606,\n",
            "         6760,  3979,  4400,  1076,     4,  2156,  6193, 25606, 14687,   219,\n",
            "          677,   260,  1638,     8, 13880,  2156,  5155, 25606, 19443,  9649,\n",
            "          329,  4400,  1076,     4,  2156,  6193, 25606,   255, 40435,  1636,\n",
            "        18002,     8, 19443,  9649,   329,  2156,  3788,  4839,   479,     2,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1])\n",
            "tensor(0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating a dataloader for both training and test datasets\n",
        "- Batch size set as indicated in the paper"
      ],
      "metadata": {
        "id": "YIC1gqWHiauv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xigcjb4nQ5wl"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
        "\n",
        "batch_size = 16\n",
        "\n",
        "train_dataset = TensorDataset(encoded_trainX, encoded_trainY)\n",
        "train_dataloader = DataLoader(\n",
        "            train_dataset,\n",
        "            sampler = RandomSampler(train_dataset),\n",
        "            batch_size = batch_size\n",
        "        )\n",
        "\n",
        "test_dataset = TensorDataset(encoded_testX, encoded_testY)\n",
        "test_dataloader = DataLoader(\n",
        "            test_dataset,\n",
        "            sampler = RandomSampler(test_dataset),\n",
        "            batch_size = batch_size\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import RobertaModel, RobertaForSequenceClassification\n",
        "from torch import nn\n",
        "\n",
        "# RobertaForSequenceClassification could also be used.\n",
        "# Drop out rate as used in the paper\n",
        "class CustomRoberta(nn.Module):\n",
        "    def __init__(self, num_classes=num_classes):\n",
        "          super(CustomRoberta, self).__init__()\n",
        "          self.robert = RobertaModel.from_pretrained(\"roberta-base\", output_attentions = True, output_hidden_states = True)\n",
        "          self.linear = nn.Linear(768, num_classes)\n",
        "          self.dropout = nn.Dropout(0.1)\n",
        "          self.activation = nn.Tanh()\n",
        "\n",
        "    def forward(self, ids):\n",
        "          # index 1 represents the pooled_output, the cls token.\n",
        "          sequence_output = self.robert(ids)[1]\n",
        "          \n",
        "          linear_output = self.linear(sequence_output)\n",
        "          dropout = self.dropout(linear_output)\n",
        "          output = self.activation(dropout)\n",
        "\n",
        "          return output\n",
        "\n",
        "model = CustomRoberta(num_classes=num_classes)\n",
        "model.cuda()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQQBIQ1WXaWd",
        "outputId": "8a945225-43c6-4a98-e682-315830792e02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CustomRoberta(\n",
              "  (robert): RobertaModel(\n",
              "    (embeddings): RobertaEmbeddings(\n",
              "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
              "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
              "      (token_type_embeddings): Embedding(1, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): RobertaEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): RobertaPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (linear): Linear(in_features=768, out_features=6, bias=True)\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (activation): Tanh()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# from Umang's code -- Note don't use Frequent Direction --> Buffer size is too large\n",
        "class FrequentDirectionAccountant:\n",
        "    \"\"\"\n",
        "    Frequent Directions algorithm (Alg 2 from the paper) for streaming SVD.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, k, l, n, device):\n",
        "        \"\"\"\n",
        "        :param k: number of eigen vectors we want eventually (k should be less than l+1)\n",
        "        :param l: buffer size\n",
        "        :param n: number of parameters/dimension of vector\n",
        "        :param device:\n",
        "        \"\"\"\n",
        "        self.K = k\n",
        "        self.L = l\n",
        "        self.N = n\n",
        "\n",
        "        self.step = 0\n",
        "        self.buffer = torch.zeros(self.L, self.N, device=device)\n",
        "\n",
        "    def update(self, vector):\n",
        "        \"\"\"\n",
        "        run one step of Freq Direction\n",
        "        :param vector:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "\n",
        "        self.buffer[self.L - 1] = vector\n",
        "        _, S, Vt = torch.linalg.svd(self.buffer, full_matrices=False)\n",
        "        delta = S[-1] ** 2\n",
        "        new_svd_vals = torch.sqrt(torch.clip(S ** 2 - delta, min=0, max=None))\n",
        "        self.buffer = torch.diag(new_svd_vals) @ Vt\n",
        "        self.step += 1\n",
        "\n",
        "    def get_current_buffer(self):\n",
        "        return self.buffer\n",
        "\n",
        "    def get_current_directions(self):\n",
        "        \"\"\"return top k eigen vectors of A^TA\"\"\"\n",
        "        _, _, Vt_B = torch.linalg.svd(self.buffer, full_matrices=False)\n",
        "        return Vt_B[:self.K]\n",
        "\n",
        "    def get_current_buffer(self):\n",
        "        return self.buffer\n",
        "\n",
        "    def get_current_directions(self):\n",
        "        \"\"\"return top k eigen vectors of A^TA\"\"\"\n",
        "        _, _, Vt_B = torch.linalg.svd(self.buffer, full_matrices=False)\n",
        "        return Vt_B[:self.K]\n",
        "\n",
        "def count_params(model: torch.nn.Module, skip_bn_bias=False):\n",
        "    count = 0\n",
        "    for param in model.parameters():\n",
        "        if param.requires_grad:\n",
        "            if param.dim() <= 1 and skip_bn_bias:\n",
        "                pass\n",
        "            else:\n",
        "                count += param.numel()\n",
        "    return count\n",
        "\n",
        "def flatten_grads(model, num_params, skip_bn_bias=False):\n",
        "    flat_grads = torch.zeros(num_params, requires_grad=False)\n",
        "    idx = 0\n",
        "    for param in model.parameters():\n",
        "        if param.requires_grad:\n",
        "            if param.dim() <= 1 and skip_bn_bias:\n",
        "                pass\n",
        "            else:\n",
        "                flat_grads[idx:idx + param.numel()] = torch.flatten(param.grad).data.cpu()\n",
        "                idx += param.numel()\n",
        "    return flat_grads\n",
        "\n",
        "def get_loss_value(model, loader, device):\n",
        "    \"\"\"\n",
        "    Evaluation loop for the multi-class classification problem.\n",
        "    return (loss, accuracy)\n",
        "    \"\"\"\n",
        "\n",
        "    model.eval()\n",
        "    losses = []\n",
        "    accuracies = []\n",
        "    with torch.no_grad():\n",
        "        for i, (images, labels) in enumerate(loader):\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(images)\n",
        "            loss = torch.nn.functional.cross_entropy(outputs, labels, reduce=None).detach()\n",
        "            losses.append(loss.reshape(-1))\n",
        "\n",
        "            acc = (torch.argmax(outputs, dim=1) == labels).float().detach()\n",
        "            accuracies.append(acc.reshape(-1))\n",
        "\n",
        "        losses = torch.cat(losses, dim=0).mean().cpu().data.numpy()\n",
        "        accuracies = torch.cat(accuracies, dim=0).mean().cpu().data.numpy()\n",
        "        return losses, accuracies"
      ],
      "metadata": {
        "id": "9gJ2Tn6maNuH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-tuning raw RoBERTa"
      ],
      "metadata": {
        "id": "txbixdiF4gtf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import dill\n",
        "import os\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import AdamW\n",
        "\n",
        "# hyperparameters as set by the paper\n",
        "epochs = 10\n",
        "optimizer = AdamW(model.parameters(), lr = 2e-5)\n",
        "\n",
        "RESULT_FOLDER = \"/content/drive/MyDrive/CS699/data/ROBERTA/\"\n",
        "os.makedirs(f\"{RESULT_FOLDER}/ckpt\", exist_ok=True)\n",
        "\n",
        "device = torch.device(\"cuda\")\n",
        "#total_params = count_params(model, skip_bn_bias=True)\n",
        "#fd = FrequentDirectionAccountant(k=2, l=10, n=total_params, device=device)\n",
        "t0 = time.time()\n",
        "for epoch in range(epochs):\n",
        "  model.train()\n",
        "\n",
        "  for i, batch in enumerate(train_dataloader):\n",
        "\n",
        "    d_input_id = batch[0].to(device)\n",
        "    d_labels = batch[1].to(device)\n",
        "    outputs = model(d_input_id)\n",
        "    loss = torch.nn.functional.cross_entropy(outputs, d_labels)\n",
        "\n",
        "    model.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    #fd.update(flatten_grads(model, total_params, skip_bn_bias=True))\n",
        "\n",
        "  loss, acc = get_loss_value(model, test_dataloader, device=device)\n",
        "  print(loss, acc)\n",
        "  torch.save(\n",
        "      model.state_dict(), f'{RESULT_FOLDER}/ckpt/{epoch + 1}_model.pt',\n",
        "      pickle_module=dill\n",
        "  )\n",
        "\n",
        "training_time = time.time() - t0\n",
        "#buffer = fd.get_current_buffer()\n",
        "#directions = fd.get_current_directions()\n",
        "#directions = directions.cpu().data.numpy()\n",
        "'''\n",
        "np.savez(\n",
        "    f\"{RESULT_FOLDER}/buffer.npy\",\n",
        "    buffer=buffer.cpu().data.numpy(), direction1=directions[0], direction2=directions[1]\n",
        ")\n",
        "'''\n",
        "\n",
        "print(training_time)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CLbiVSn49CU9",
        "outputId": "01ac193f-f5b6-4a98-f482-804c61306052"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.2384826 0.6115108\n",
            "1.2666544 0.5827338\n",
            "1.3573583 0.5467626\n",
            "1.3387839 0.55395687\n",
            "1.2878712 0.5827338\n",
            "1.3817556 0.51079136\n",
            "1.3753235 0.51079136\n",
            "1.3729696 0.51079136\n",
            "1.3826815 0.51079136\n",
            "1.3739272 0.51079136\n",
            "970.9645256996155\n"
          ]
        }
      ]
    }
  ]
}