{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F51oYwCRFr5g",
        "outputId": "a82838f1-de3b-4aa8-8cd1-0577dea28228"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: wilds in /usr/local/lib/python3.7/dist-packages (2.0.0)\n",
            "Requirement already satisfied: ogb>=1.2.6 in /usr/local/lib/python3.7/dist-packages (from wilds) (1.3.3)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.7/dist-packages (from wilds) (1.0.2)\n",
            "Requirement already satisfied: numpy>=1.19.1 in /usr/local/lib/python3.7/dist-packages (from wilds) (1.21.6)\n",
            "Requirement already satisfied: scipy>=1.5.4 in /usr/local/lib/python3.7/dist-packages (from wilds) (1.7.3)\n",
            "Requirement already satisfied: outdated>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from wilds) (0.2.1)\n",
            "Requirement already satisfied: torch>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from wilds) (1.10.0+cu111)\n",
            "Requirement already satisfied: pandas>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from wilds) (1.3.5)\n",
            "Requirement already satisfied: pytz>=2020.4 in /usr/local/lib/python3.7/dist-packages (from wilds) (2022.1)\n",
            "Requirement already satisfied: pillow>=7.2.0 in /usr/local/lib/python3.7/dist-packages (from wilds) (9.1.0)\n",
            "Requirement already satisfied: torchvision>=0.8.2 in /usr/local/lib/python3.7/dist-packages (from wilds) (0.11.1+cu111)\n",
            "Requirement already satisfied: tqdm>=4.53.0 in /usr/local/lib/python3.7/dist-packages (from wilds) (4.64.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from ogb>=1.2.6->wilds) (1.15.0)\n",
            "Requirement already satisfied: urllib3>=1.24.0 in /usr/local/lib/python3.7/dist-packages (from ogb>=1.2.6->wilds) (1.24.3)\n",
            "Requirement already satisfied: littleutils in /usr/local/lib/python3.7/dist-packages (from outdated>=0.2.0->wilds) (0.2.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from outdated>=0.2.0->wilds) (2.23.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.1.0->wilds) (2.8.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->wilds) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->wilds) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.7.0->wilds) (4.1.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->outdated>=0.2.0->wilds) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->outdated>=0.2.0->wilds) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->outdated>=0.2.0->wilds) (3.0.4)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.18.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.49)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.5.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install wilds\n",
        "!pip install transformers\n",
        "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-1.10.0+cu113.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N29SZtbaPWeK"
      },
      "outputs": [],
      "source": [
        "import dill\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn import preprocessing\n",
        "import torch\n",
        "from torch import nn\n",
        "from transformers import RobertaModel, RobertaTokenizer\n",
        "from torch.optim import AdamW\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sHRQ9J_wFxLj"
      },
      "outputs": [],
      "source": [
        "from wilds import get_dataset\n",
        "\n",
        "dataset = get_dataset(dataset=\"civilcomments\", download=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OkZxxo4IPZKa"
      },
      "outputs": [],
      "source": [
        "train = dataset.get_subset(\"train\")\n",
        "test = dataset.get_subset(\"test\")\n",
        "\n",
        "trainX = [data[0] for data in train]\n",
        "trainY = torch.stack(([data[1] for data in train]))\n",
        "pudd\n",
        "testX = [data[0] for data in test]\n",
        "testY = torch.stack(([data[1] for data in test]))\n",
        "testMeta = torch.stack(([data[2] for data in test]))\n",
        "\n",
        "val = dataset.get_subset('val')\n",
        "valX = [data[0] for data in val]\n",
        "valY = [data[1] for data in val]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yuC-o9QOTuqV"
      },
      "outputs": [],
      "source": [
        "# distilroberta-base \n",
        "# mrm8488/distilroberta-finetuned-tweets-hate-speech\n",
        "\n",
        "def convert_txt2tokenid(tokenizer, text):\n",
        "  \"\"\"\n",
        "  Uses the loaded tokenizer from HuggingFace to tokenize the raw text into int index\n",
        "  return tokenized torch array\n",
        "  \"\"\"\n",
        "  token_ids = []\n",
        "  for sent in text:\n",
        "    tokens = tokenizer.encode(sent, truncation=True, padding='max_length', return_tensors = 'pt')\n",
        "    token_ids.append(tokens)\n",
        "  return torch.cat(token_ids, dim=0)\n",
        "\n",
        "# Tokenizers used in the domain adapted versions of RoBERTa are identical to roberta-base\n",
        "roberta_tokenizer = RobertaTokenizer.from_pretrained(\"mrm8488/distilroberta-finetuned-tweets-hate-speech\")\n",
        "\n",
        "encoded_trainX = convert_txt2tokenid(roberta_tokenizer, trainX)\n",
        "encoded_testX = convert_txt2tokenid(roberta_tokenizer, testX)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EtKqMeESYZkS"
      },
      "outputs": [],
      "source": [
        "batch_size = 16\n",
        "\n",
        "train_dataset = TensorDataset(encoded_trainX, trainY)\n",
        "train_dataloader = DataLoader(\n",
        "            train_dataset,\n",
        "            sampler = RandomSampler(train_dataset),\n",
        "            batch_size = batch_size\n",
        "        )\n",
        "\n",
        "test_dataset = TensorDataset(encoded_testX, testY, testMeta)\n",
        "test_dataloader = DataLoader(\n",
        "            test_dataset,\n",
        "            batch_size = batch_size\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zvzBZNPuIYe-",
        "outputId": "1455cb0d-b2db-4faa-fa16-36c7c66fdde5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at mrm8488/distilroberta-finetuned-tweets-hate-speech were not used when initializing RobertaModel: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at mrm8488/distilroberta-finetuned-tweets-hate-speech and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "CustomRoberta(\n",
              "  (robert): RobertaModel(\n",
              "    (embeddings): RobertaEmbeddings(\n",
              "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
              "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
              "      (token_type_embeddings): Embedding(1, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): RobertaEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): RobertaPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (linear): Linear(in_features=768, out_features=2, bias=True)\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (activation): Tanh()\n",
              ")"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from torch import nn\n",
        "from transformers import RobertaModel, RobertaTokenizer\n",
        "\n",
        "# RobertaForSequenceClassification could also be used.\n",
        "# Drop out rate as used in the paper\n",
        "class CustomRoberta(nn.Module):\n",
        "    def __init__(self):\n",
        "          super(CustomRoberta, self).__init__()\n",
        "          self.robert = RobertaModel.from_pretrained(\"mrm8488/distilroberta-finetuned-tweets-hate-speech\", output_attentions = True, output_hidden_states = True)\n",
        "          self.linear = nn.Linear(768, 2)\n",
        "          self.dropout = nn.Dropout(0.1)\n",
        "          self.activation = nn.Tanh()\n",
        "\n",
        "    def forward(self, ids):\n",
        "          # index 1 represents the pooled_output, the cls token.\n",
        "          sequence_output = self.robert(ids)[1]\n",
        "          \n",
        "          linear_output = self.linear(sequence_output)\n",
        "          dropout = self.dropout(linear_output)\n",
        "          output = self.activation(dropout)\n",
        "\n",
        "          return output\n",
        "\n",
        "model = CustomRoberta()\n",
        "model.cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B0r60JuNbEGh"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(42)\n",
        "epochs = 5\n",
        "optimizer = AdamW(model.parameters(), lr = 1e-5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vQUt1YgOc7YN"
      },
      "outputs": [],
      "source": [
        "def get_loss_value(model, loader, device, cal_f1=True, benchmark_val=False):\n",
        "    \"\"\"\n",
        "    Evaluation loop for the multi-class classification problem.\n",
        "    return (loss, accuracy)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    losses = []\n",
        "    accuracies = []\n",
        "    pred_labels = []\n",
        "    true_labels = []\n",
        "    meta_info = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (images, labels, meta) in enumerate(loader):\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            \n",
        "            # Forward pass\n",
        "            outputs = model(images)\n",
        "            loss = torch.nn.functional.cross_entropy(outputs, labels, reduce=None).detach()\n",
        "            losses.append(loss.reshape(-1))\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "            acc = (preds == labels).float().detach()\n",
        "            pred_labels+=preds.detach().cpu().tolist()\n",
        "            true_labels+=labels.detach().cpu().tolist()\n",
        "            accuracies.append(acc.reshape(-1))\n",
        "            meta_info.append(meta)\n",
        "\n",
        "        if benchmark_val:\n",
        "          return torch.FloatTensor(pred_labels), torch.FloatTensor(true_labels), torch.cat(meta_info, dim=0)\n",
        "\n",
        "        losses = torch.cat(losses, dim=0).mean().cpu().data.numpy()\n",
        "        accuracies = torch.cat(accuracies, dim=0).mean().cpu().data.numpy()\n",
        "\n",
        "        ## As the original paper used the macro F1 score to evaluate the fine-tuned models\n",
        "        ## additional argument (cal_f1) defined to calculate macro F1 score within this function\n",
        "        if cal_f1:\n",
        "          p_macro, r_macro, f1_macro, support_macro = \\\n",
        "                  precision_recall_fscore_support(y_true=np.array(true_labels), y_pred=np.array(pred_labels), average='macro')\n",
        "          return losses, accuracies, p_macro, r_macro, f1_macro\n",
        "        else:\n",
        "          return losses, accuracies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "eKnOiiNipDoH",
        "outputId": "20834f30-062c-4f9c-b152-426a3dc99e3f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "./drive/MyDrive/CS699/homework #3/distilroberta_hate_speech_backup/1_model.pt\n",
            "({'acc_avg': 0.923158586025238, 'acc_y:0_male:1': 0.9447568655014038, 'count_y:0_male:1': 12092.0, 'acc_y:1_male:1': 0.5760326981544495, 'count_y:1_male:1': 2203.0, 'acc_y:0_female:1': 0.9560617804527283, 'count_y:0_female:1': 14179.0, 'acc_y:1_female:1': 0.5616739988327026, 'count_y:1_female:1': 2270.0, 'acc_y:0_LGBTQ:1': 0.9031152725219727, 'count_y:0_LGBTQ:1': 3210.0, 'acc_y:1_LGBTQ:1': 0.5254934430122375, 'count_y:1_LGBTQ:1': 1216.0, 'acc_y:0_christian:1': 0.9742169976234436, 'count_y:0_christian:1': 12101.0, 'acc_y:1_christian:1': 0.4793650805950165, 'count_y:1_christian:1': 1260.0, 'acc_y:0_muslim:1': 0.9084967374801636, 'count_y:0_muslim:1': 5355.0, 'acc_y:1_muslim:1': 0.5556238293647766, 'count_y:1_muslim:1': 1627.0, 'acc_y:0_other_religions:1': 0.9442952871322632, 'count_y:0_other_religions:1': 2980.0, 'acc_y:1_other_religions:1': 0.5538461804389954, 'count_y:1_other_religions:1': 520.0, 'acc_y:0_black:1': 0.8683658242225647, 'count_y:0_black:1': 3335.0, 'acc_y:1_black:1': 0.6343526244163513, 'count_y:1_black:1': 1537.0, 'acc_y:0_white:1': 0.8726192712783813, 'count_y:0_white:1': 5723.0, 'acc_y:1_white:1': 0.62600177526474, 'count_y:1_white:1': 2246.0, 'acc_wg': 0.4793650805950165}, 'Average acc: 0.923\\n  male                   acc on non_toxic: 0.945 (n =  12092)    acc on toxic: 0.576 (n =   2203) \\n  female                 acc on non_toxic: 0.956 (n =  14179)    acc on toxic: 0.562 (n =   2270) \\n  LGBTQ                  acc on non_toxic: 0.903 (n =   3210)    acc on toxic: 0.525 (n =   1216) \\n  christian              acc on non_toxic: 0.974 (n =  12101)    acc on toxic: 0.479 (n =   1260) \\n  muslim                 acc on non_toxic: 0.908 (n =   5355)    acc on toxic: 0.556 (n =   1627) \\n  other_religions        acc on non_toxic: 0.944 (n =   2980)    acc on toxic: 0.554 (n =    520) \\n  black                  acc on non_toxic: 0.868 (n =   3335)    acc on toxic: 0.634 (n =   1537) \\n  white                  acc on non_toxic: 0.873 (n =   5723)    acc on toxic: 0.626 (n =   2246) \\nWorst-group acc: 0.479\\n')\n",
            "--------------------------\n"
          ]
        }
      ],
      "source": [
        "import glob\n",
        "\n",
        "model_path = glob.glob(\"./drive/MyDrive/CS699/homework #3/distilroberta_hate_speech_backup/*\")\n",
        "\n",
        "device = torch.device(\"cuda\")\n",
        "\n",
        "def load_ckp(checkpoint_fpath, model):\n",
        "    checkpoint = torch.load(checkpoint_fpath)\n",
        "    model.load_state_dict(checkpoint)\n",
        "    return model\n",
        "\n",
        "for ckp_path in model_path:\n",
        "  print(ckp_path)\n",
        "  model = load_ckp(ckp_path, model)\n",
        "  #loss, acc, prec, recall, f1 = get_loss_value(model, test_dataloader, device=device, benchmark_val=True)\n",
        "  pred, label, meta = get_loss_value(model, test_dataloader, device=device, benchmark_val=True)\n",
        "  #print(\"\\t Loss: %f, Accuracy on the test dataset: %f\" %(loss, acc))\n",
        "  #print(\"\\t prec: %f, recall: %f, macro f1: %f\" %(prec, recall, f1))\n",
        "  print(dataset.eval(pred, label, meta))\n",
        "  print('--------------------------')\n",
        "  break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "WeyDkSFbcQv7",
        "outputId": "08616fdd-fd96-4760-815a-2398a9bb78e4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 20%|██        | 16815/84075 [2:01:23<7:45:30,  2.41it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "({'acc_avg': 0.9200714826583862, 'acc_y:0_male:1': 0.92449551820755, 'count_y:0_male:1': 12092.0, 'acc_y:1_male:1': 0.6631865501403809, 'count_y:1_male:1': 2203.0, 'acc_y:0_female:1': 0.941392183303833, 'count_y:0_female:1': 14179.0, 'acc_y:1_female:1': 0.6356828212738037, 'count_y:1_female:1': 2270.0, 'acc_y:0_LGBTQ:1': 0.8314641714096069, 'count_y:0_LGBTQ:1': 3210.0, 'acc_y:1_LGBTQ:1': 0.6842105388641357, 'count_y:1_LGBTQ:1': 1216.0, 'acc_y:0_christian:1': 0.9619866013526917, 'count_y:0_christian:1': 12101.0, 'acc_y:1_christian:1': 0.5571428537368774, 'count_y:1_christian:1': 1260.0, 'acc_y:0_muslim:1': 0.8802987933158875, 'count_y:0_muslim:1': 5355.0, 'acc_y:1_muslim:1': 0.63368159532547, 'count_y:1_muslim:1': 1627.0, 'acc_y:0_other_religions:1': 0.9369127750396729, 'count_y:0_other_religions:1': 2980.0, 'acc_y:1_other_religions:1': 0.5923076868057251, 'count_y:1_other_religions:1': 520.0, 'acc_y:0_black:1': 0.830584704875946, 'count_y:0_black:1': 3335.0, 'acc_y:1_black:1': 0.6974626183509827, 'count_y:1_black:1': 1537.0, 'acc_y:0_white:1': 0.8427398204803467, 'count_y:0_white:1': 5723.0, 'acc_y:1_white:1': 0.6780943870544434, 'count_y:1_white:1': 2246.0, 'acc_wg': 0.5571428537368774}, 'Average acc: 0.920\\n  male                   acc on non_toxic: 0.924 (n =  12092)    acc on toxic: 0.663 (n =   2203) \\n  female                 acc on non_toxic: 0.941 (n =  14179)    acc on toxic: 0.636 (n =   2270) \\n  LGBTQ                  acc on non_toxic: 0.831 (n =   3210)    acc on toxic: 0.684 (n =   1216) \\n  christian              acc on non_toxic: 0.962 (n =  12101)    acc on toxic: 0.557 (n =   1260) \\n  muslim                 acc on non_toxic: 0.880 (n =   5355)    acc on toxic: 0.634 (n =   1627) \\n  other_religions        acc on non_toxic: 0.937 (n =   2980)    acc on toxic: 0.592 (n =    520) \\n  black                  acc on non_toxic: 0.831 (n =   3335)    acc on toxic: 0.697 (n =   1537) \\n  white                  acc on non_toxic: 0.843 (n =   5723)    acc on toxic: 0.678 (n =   2246) \\nWorst-group acc: 0.557\\n')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 40%|████      | 33630/84075 [4:23:10<5:50:23,  2.40it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "({'acc_avg': 0.9275911450386047, 'acc_y:0_male:1': 0.9594773650169373, 'count_y:0_male:1': 12092.0, 'acc_y:1_male:1': 0.532001793384552, 'count_y:1_male:1': 2203.0, 'acc_y:0_female:1': 0.966640830039978, 'count_y:0_female:1': 14179.0, 'acc_y:1_female:1': 0.5400881171226501, 'count_y:1_female:1': 2270.0, 'acc_y:0_LGBTQ:1': 0.9193146228790283, 'count_y:0_LGBTQ:1': 3210.0, 'acc_y:1_LGBTQ:1': 0.5189144611358643, 'count_y:1_LGBTQ:1': 1216.0, 'acc_y:0_christian:1': 0.9790926575660706, 'count_y:0_christian:1': 12101.0, 'acc_y:1_christian:1': 0.45793649554252625, 'count_y:1_christian:1': 1260.0, 'acc_y:0_muslim:1': 0.936881422996521, 'count_y:0_muslim:1': 5355.0, 'acc_y:1_muslim:1': 0.5070682168006897, 'count_y:1_muslim:1': 1627.0, 'acc_y:0_other_religions:1': 0.9624161124229431, 'count_y:0_other_religions:1': 2980.0, 'acc_y:1_other_religions:1': 0.5057692527770996, 'count_y:1_other_religions:1': 520.0, 'acc_y:0_black:1': 0.9142428636550903, 'count_y:0_black:1': 3335.0, 'acc_y:1_black:1': 0.5640859007835388, 'count_y:1_black:1': 1537.0, 'acc_y:0_white:1': 0.9215446710586548, 'count_y:0_white:1': 5723.0, 'acc_y:1_white:1': 0.5387355089187622, 'count_y:1_white:1': 2246.0, 'acc_wg': 0.45793649554252625}, 'Average acc: 0.928\\n  male                   acc on non_toxic: 0.959 (n =  12092)    acc on toxic: 0.532 (n =   2203) \\n  female                 acc on non_toxic: 0.967 (n =  14179)    acc on toxic: 0.540 (n =   2270) \\n  LGBTQ                  acc on non_toxic: 0.919 (n =   3210)    acc on toxic: 0.519 (n =   1216) \\n  christian              acc on non_toxic: 0.979 (n =  12101)    acc on toxic: 0.458 (n =   1260) \\n  muslim                 acc on non_toxic: 0.937 (n =   5355)    acc on toxic: 0.507 (n =   1627) \\n  other_religions        acc on non_toxic: 0.962 (n =   2980)    acc on toxic: 0.506 (n =    520) \\n  black                  acc on non_toxic: 0.914 (n =   3335)    acc on toxic: 0.564 (n =   1537) \\n  white                  acc on non_toxic: 0.922 (n =   5723)    acc on toxic: 0.539 (n =   2246) \\nWorst-group acc: 0.458\\n')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 60%|██████    | 50445/84075 [6:45:01<3:53:31,  2.40it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "({'acc_avg': 0.9258420467376709, 'acc_y:0_male:1': 0.9444260597229004, 'count_y:0_male:1': 12092.0, 'acc_y:1_male:1': 0.6082614660263062, 'count_y:1_male:1': 2203.0, 'acc_y:0_female:1': 0.9585302472114563, 'count_y:0_female:1': 14179.0, 'acc_y:1_female:1': 0.5969163179397583, 'count_y:1_female:1': 2270.0, 'acc_y:0_LGBTQ:1': 0.9233644604682922, 'count_y:0_LGBTQ:1': 3210.0, 'acc_y:1_LGBTQ:1': 0.49424341320991516, 'count_y:1_LGBTQ:1': 1216.0, 'acc_y:0_christian:1': 0.9711593985557556, 'count_y:0_christian:1': 12101.0, 'acc_y:1_christian:1': 0.5007936358451843, 'count_y:1_christian:1': 1260.0, 'acc_y:0_muslim:1': 0.9305322170257568, 'count_y:0_muslim:1': 5355.0, 'acc_y:1_muslim:1': 0.5365703701972961, 'count_y:1_muslim:1': 1627.0, 'acc_y:0_other_religions:1': 0.9563758373260498, 'count_y:0_other_religions:1': 2980.0, 'acc_y:1_other_religions:1': 0.5346153974533081, 'count_y:1_other_religions:1': 520.0, 'acc_y:0_black:1': 0.8896551728248596, 'count_y:0_black:1': 3335.0, 'acc_y:1_black:1': 0.6265451908111572, 'count_y:1_black:1': 1537.0, 'acc_y:0_white:1': 0.8907915353775024, 'count_y:0_white:1': 5723.0, 'acc_y:1_white:1': 0.6050757169723511, 'count_y:1_white:1': 2246.0, 'acc_wg': 0.49424341320991516}, 'Average acc: 0.926\\n  male                   acc on non_toxic: 0.944 (n =  12092)    acc on toxic: 0.608 (n =   2203) \\n  female                 acc on non_toxic: 0.959 (n =  14179)    acc on toxic: 0.597 (n =   2270) \\n  LGBTQ                  acc on non_toxic: 0.923 (n =   3210)    acc on toxic: 0.494 (n =   1216) \\n  christian              acc on non_toxic: 0.971 (n =  12101)    acc on toxic: 0.501 (n =   1260) \\n  muslim                 acc on non_toxic: 0.931 (n =   5355)    acc on toxic: 0.537 (n =   1627) \\n  other_religions        acc on non_toxic: 0.956 (n =   2980)    acc on toxic: 0.535 (n =    520) \\n  black                  acc on non_toxic: 0.890 (n =   3335)    acc on toxic: 0.627 (n =   1537) \\n  white                  acc on non_toxic: 0.891 (n =   5723)    acc on toxic: 0.605 (n =   2246) \\nWorst-group acc: 0.494\\n')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 80%|████████  | 67260/84075 [9:06:54<1:56:31,  2.41it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "({'acc_avg': 0.922844648361206, 'acc_y:0_male:1': 0.9341713786125183, 'count_y:0_male:1': 12092.0, 'acc_y:1_male:1': 0.6441216468811035, 'count_y:1_male:1': 2203.0, 'acc_y:0_female:1': 0.9456238150596619, 'count_y:0_female:1': 14179.0, 'acc_y:1_female:1': 0.6334801912307739, 'count_y:1_female:1': 2270.0, 'acc_y:0_LGBTQ:1': 0.8598130941390991, 'count_y:0_LGBTQ:1': 3210.0, 'acc_y:1_LGBTQ:1': 0.6365131735801697, 'count_y:1_LGBTQ:1': 1216.0, 'acc_y:0_christian:1': 0.963556706905365, 'count_y:0_christian:1': 12101.0, 'acc_y:1_christian:1': 0.5452380776405334, 'count_y:1_christian:1': 1260.0, 'acc_y:0_muslim:1': 0.8898226022720337, 'count_y:0_muslim:1': 5355.0, 'acc_y:1_muslim:1': 0.627535343170166, 'count_y:1_muslim:1': 1627.0, 'acc_y:0_other_religions:1': 0.9409396052360535, 'count_y:0_other_religions:1': 2980.0, 'acc_y:1_other_religions:1': 0.6000000238418579, 'count_y:1_other_religions:1': 520.0, 'acc_y:0_black:1': 0.8179910182952881, 'count_y:0_black:1': 3335.0, 'acc_y:1_black:1': 0.7358490824699402, 'count_y:1_black:1': 1537.0, 'acc_y:0_white:1': 0.8514764904975891, 'count_y:0_white:1': 5723.0, 'acc_y:1_white:1': 0.6731967926025391, 'count_y:1_white:1': 2246.0, 'acc_wg': 0.5452380776405334}, 'Average acc: 0.923\\n  male                   acc on non_toxic: 0.934 (n =  12092)    acc on toxic: 0.644 (n =   2203) \\n  female                 acc on non_toxic: 0.946 (n =  14179)    acc on toxic: 0.633 (n =   2270) \\n  LGBTQ                  acc on non_toxic: 0.860 (n =   3210)    acc on toxic: 0.637 (n =   1216) \\n  christian              acc on non_toxic: 0.964 (n =  12101)    acc on toxic: 0.545 (n =   1260) \\n  muslim                 acc on non_toxic: 0.890 (n =   5355)    acc on toxic: 0.628 (n =   1627) \\n  other_religions        acc on non_toxic: 0.941 (n =   2980)    acc on toxic: 0.600 (n =    520) \\n  black                  acc on non_toxic: 0.818 (n =   3335)    acc on toxic: 0.736 (n =   1537) \\n  white                  acc on non_toxic: 0.851 (n =   5723)    acc on toxic: 0.673 (n =   2246) \\nWorst-group acc: 0.545\\n')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 84075/84075 [11:28:43<00:00,  2.41it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "({'acc_avg': 0.925834596157074, 'acc_y:0_male:1': 0.9496361017227173, 'count_y:0_male:1': 12092.0, 'acc_y:1_male:1': 0.5601452589035034, 'count_y:1_male:1': 2203.0, 'acc_y:0_female:1': 0.9623386859893799, 'count_y:0_female:1': 14179.0, 'acc_y:1_female:1': 0.5414096713066101, 'count_y:1_female:1': 2270.0, 'acc_y:0_LGBTQ:1': 0.9059190154075623, 'count_y:0_LGBTQ:1': 3210.0, 'acc_y:1_LGBTQ:1': 0.5222039222717285, 'count_y:1_LGBTQ:1': 1216.0, 'acc_y:0_christian:1': 0.9737212061882019, 'count_y:0_christian:1': 12101.0, 'acc_y:1_christian:1': 0.4611110985279083, 'count_y:1_christian:1': 1260.0, 'acc_y:0_muslim:1': 0.9297852516174316, 'count_y:0_muslim:1': 5355.0, 'acc_y:1_muslim:1': 0.5230485796928406, 'count_y:1_muslim:1': 1627.0, 'acc_y:0_other_religions:1': 0.9570469856262207, 'count_y:0_other_religions:1': 2980.0, 'acc_y:1_other_religions:1': 0.5192307829856873, 'count_y:1_other_religions:1': 520.0, 'acc_y:0_black:1': 0.8707646131515503, 'count_y:0_black:1': 3335.0, 'acc_y:1_black:1': 0.6356538534164429, 'count_y:1_black:1': 1537.0, 'acc_y:0_white:1': 0.9070417881011963, 'count_y:0_white:1': 5723.0, 'acc_y:1_white:1': 0.5467497706413269, 'count_y:1_white:1': 2246.0, 'acc_wg': 0.4611110985279083}, 'Average acc: 0.926\\n  male                   acc on non_toxic: 0.950 (n =  12092)    acc on toxic: 0.560 (n =   2203) \\n  female                 acc on non_toxic: 0.962 (n =  14179)    acc on toxic: 0.541 (n =   2270) \\n  LGBTQ                  acc on non_toxic: 0.906 (n =   3210)    acc on toxic: 0.522 (n =   1216) \\n  christian              acc on non_toxic: 0.974 (n =  12101)    acc on toxic: 0.461 (n =   1260) \\n  muslim                 acc on non_toxic: 0.930 (n =   5355)    acc on toxic: 0.523 (n =   1627) \\n  other_religions        acc on non_toxic: 0.957 (n =   2980)    acc on toxic: 0.519 (n =    520) \\n  black                  acc on non_toxic: 0.871 (n =   3335)    acc on toxic: 0.636 (n =   1537) \\n  white                  acc on non_toxic: 0.907 (n =   5723)    acc on toxic: 0.547 (n =   2246) \\nWorst-group acc: 0.461\\n')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r100%|██████████| 84075/84075 [11:49:06<00:00,  1.98it/s]\n"
          ]
        }
      ],
      "source": [
        "RESULT_FOLDER = \"./drive/MyDrive/CS699/homework #3\"\n",
        "os.makedirs(f\"{RESULT_FOLDER}/distilroberta-finetuned-tweets-hate-speech/\", exist_ok=True)\n",
        "\n",
        "device = torch.device(\"cuda\")\n",
        "\n",
        "with tqdm(total=epochs*len(train_dataloader)) as pbar:\n",
        "  for epoch in range(epochs):\n",
        "    model.train()\n",
        "\n",
        "    for i, batch in enumerate(train_dataloader):\n",
        "\n",
        "      d_input_id = batch[0].to(device)\n",
        "      d_labels = batch[1].to(device)\n",
        "      outputs = model(d_input_id)\n",
        "      loss = torch.nn.functional.cross_entropy(outputs, d_labels)\n",
        "\n",
        "      model.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      pbar.update(1)\n",
        "\n",
        "    pred, label, meta = get_loss_value(model, test_dataloader, device=device, benchmark_val=True)\n",
        "    print(dataset.eval(pred, label, meta))\n",
        "    \n",
        "    torch.save(\n",
        "        model.state_dict(), f'{RESULT_FOLDER}/distilroberta-finetuned-tweets-hate-speech/{epoch + 1}_model.pt',\n",
        "        pickle_module=dill\n",
        "    )\n",
        "    \n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "background_execution": "on",
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "distilroberta-finetuned-tweets-hate-speech.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}